# Transformer from Scratch

Implementation of Transformer model from scratch for language modeling tasks. This project is part of the "Fundamentals and Applications of Large Models" course assignment.

## ğŸ“‹ Project Overview

This project implements a complete Transformer model from scratch, including:
- Multi-Head Self-Attention mechanism
- Position-wise Feed-Forward Networks
- Residual Connections with Layer Normalization
- Sinusoidal Positional Encoding
- Complete training pipeline with ablation studies

## ğŸ—ï¸ Project Structure
